{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classification \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most straight forward and fastest classification algorithm, suitable for large chunk of data. used in various applications such as spam filtering, text classification, sentiment analysis, and recommender systems. uses Bayes theorem of probability for prediction of unknown class.\n",
    "\n",
    "### Classification Workflow\n",
    "classification has two phases, a learning phase, and the evaluation phase. In the learning phase, classifier trains its model on a given dataset and in the evaluation phase, it tests the classifier performance. Performance is evaluated on the basis of various parameters such as accuracy, error, precision, and recall.\n",
    "\n",
    "<img src=\"classifier_workflow.png\">\n",
    "                                    src: Datacamp\n",
    "\n",
    "### Naive Bayes Classifier\n",
    " a statistical classification technique based on Bayes Theorem.Naive Bayes classifiers have high accuracy and speed on large datasets.Naive bayes Classifier assumes that the effect of a particular feature in a class is independent of other features. Even if the feauters are interdependent, naive bayes classifier still consider the feature indepndent. his assumption simplifies computation, and that's why it is considered as naive. This assumption is called class conditional independence.\n",
    " \n",
    " <img src=\"naive_eqn.png\">\n",
    "                                    src: Datacamp\n",
    "\n",
    "P(h): is the probability of class or **class prior probability**\n",
    "\n",
    "P(D): is the **prior probability of predictor**\n",
    "\n",
    "P(h|D): is the **posterior probability** of class D given predictor ( features)..\n",
    "\n",
    "P(D|h): is the **likelihood** which is the probability of predictor given class.\n",
    "\n",
    "### Naive Bayes Classifier Workflow\n",
    "#### Single Feature\n",
    "Step 1: Calculate the prior probability for given class labels\n",
    "\n",
    "Step 2: Find Likelihood probability with each attribute for each class\n",
    "\n",
    "Step 3: Put these value in Bayes Formula and calculate posterior probability.\n",
    "\n",
    "Step 4: See which class has a higher probability, given the input belongs to the higher probability class.\n",
    "\n",
    "\n",
    "### Example: Cancer Test\n",
    "Suppose a specific cancer that occur in 1% of the population and the test for this cancer and with 90% chance that its positive if you have this cancer C. and with 90% chance that its negative if you don't have this cancer C.\n",
    "\n",
    "Without further symptom take the test and test comes positive. What is the probability of having specific type of cancer?\n",
    "\n",
    "### Solution (Mathematically)\n",
    "Prioir Probabilities:\n",
    "\n",
    "P(C) = 1% = 0.01\n",
    "\n",
    "P(!C) = 99% = 0.99  \n",
    "\n",
    "P(pos | C) = 90% = 0.9\n",
    "\n",
    "P(neg | !C)= 90% = 0.9\n",
    "\n",
    "P(pos | !C) = 0.1\n",
    "\n",
    "Joint Probability:\n",
    "\n",
    "Cancer Given that test says positive\n",
    "\n",
    "P(C, pos) = P(C).P(pos|C) = 0.009\n",
    "\n",
    "No Cancer even the test says positive\n",
    "\n",
    "P(!C, pos) = P(!C).P(pos|!C) = 0.099\n",
    "\n",
    "This is about the correct equation but the probabilities don't add upto 1. To do so we need to do Normalization.\n",
    "\n",
    "Normalization:\n",
    "\n",
    "Proceeds in two steps.\n",
    "\n",
    "1. normalize the joint probability to keep the ratio the same\n",
    "\n",
    "2. Make sure the joint probability add upto 1.\n",
    "\n",
    "P(pos) = p(C,pos) + P(!C,pos) = 0.108\n",
    "\n",
    "Posterior Probability:\n",
    "\n",
    "P(C|pos) = P(C,pos)/P(pos) = 0.083\n",
    "\n",
    "P(!C|pos) = P(!C,pos)/P(pos) = 0.917\n",
    "\n",
    "Total Probability: P(C|pos) + P(!C|pos)\n",
    "\n",
    "Answer: 0.083\n",
    "##### Sensitivity - positive 90% chance.\n",
    "##### Specitivity - negative 90% chance\n",
    "\n",
    "#### Multiple Features\n",
    "Step 1: Calculate prior probability for given class label\n",
    "\n",
    "Step 2: Calculate Conditional Probability with each attribute for each class\n",
    "\n",
    "Step 3: Multiply Same class conditional probability\n",
    "\n",
    "Step 4: Multiply prior probability with step 3 probability\n",
    "\n",
    "Step 5: See which class has higher probability, higher probability class belongs to given input set step.\n",
    "\n",
    "\n",
    "### Advantages\n",
    "1. Not only simple but also fast and accurate for prediction\n",
    "\n",
    "2. Computational cost is very low\n",
    "\n",
    "3. work efficiently on large datasets\n",
    "\n",
    "4. Compared to continious variable perform well on discrete variable.\n",
    "\n",
    "5. Works well with multiple class prediction problem\n",
    "\n",
    "6. For numerical variable, normal distribution is assumed (bell curve, which is a strong assumption)\n",
    "\n",
    "7. When assumption of independence holds, a Naive Bayes classifier performs better compare to other models like logistic regression and need less training data.\n",
    "\n",
    "\n",
    "### Disadvantage\n",
    "1. Assumption of independent features, in practical it is impossible to find a set of predictors which are completely independent\n",
    "\n",
    "2. If a particular class has no training tuple, then the posterior probability becomes zero. Then the model is unable to make prediction. AKA **Zero Probability/Frequency Problem**\n",
    "\n",
    "3. naive Bayes is also known as a bad estimator\n",
    "\n",
    "### Applications\n",
    "\n",
    "#### Text classification/ Spam Filtering/ Sentiment Analysis\n",
    "Naive Bayes classifiers mostly used in text classification (due to better result in multi class problems and independence rule) have higher success rate as compared to other algorithms. As a result, it is widely used in Spam filtering (identify spam e-mail) and Sentiment Analysis (in social media analysis, to identify positive and negative customer sentiments)\n",
    "\n",
    "#### Recommendation System\n",
    "Naive Bayes Classifier and Collaborative Filtering together builds a Recommendation System that uses machine learning and data mining techniques to filter unseen information and predict whether a user would like a given resource or not.\n",
    "\n",
    "#### Multi Class Prediction or Real time Prediction\n",
    "\n",
    "### Use when:\n",
    "1. Dataset is huge\n",
    "\n",
    "2. Small Training Set\n",
    "\n",
    "3. Doing Text Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
